---
title: "h2o-autoencoders"
author: "tiangsinf"
date: "9/21/2018"
output: 
  html_document: 
    keep_md: yes
---
```{r, prelims, include=FALSE}
library(h2o)
library(tidyverse)
h2o.init()

airlines <- read.csv("dataset/allyears2k_headers.csv", header = TRUE, stringsAsFactors = FALSE)
```

### Data analysis
```{r}
summary(airlines)
str(airlines)
```
There are a total of 31 columns where 7 of those are strings and possibly factors. We will try to figure out which are the factor columns and convert them to `factor` next!

### Subset chracter columns from the airline dataset
```{r}
non_numeric_columns <- airlines[, sapply(airlines, class) == "character"]
```

### Counting number of unique values from `non_numeric_columns`
```{r}
# Function to return unique value count
unique_count <- function(x) {
    length(unique(x))
}
sapply(non_numeric_columns, unique_count) # apply function to non_numeric_columns
```
Prbably can convert `UniqueCarrier`, `Origin`, `Dest`, `ConcellationCode`, `IsArrDelayed` and `IsDepDelayed` to `factor`.

### Converting columns to factor
```{r}
n <- names(non_numeric_columns[-2])

airlines[, c(paste(n, sep = ", "))] <- as.data.frame(sapply(airlines[, c(paste(n, sep = ", "))], as.factor))
str(airlines)
```
Now we have 6 factor variables, 1 character variable ("TailNum") and 24 integer variablers

### Convert df to h2o objects
```{r}
h2o_airlines <- as.h2o(airlines)

# Separating frames to train, valid and test sets
parts <- h2o.splitFrame(h2o_airlines, 0.8, seed = 123)
train <- parts[[1]]
test <- parts[[2]]

x <- names(airlines)
```

### Apply h2o deeplearning autuencoder
```{r, 31 inputs, cache=TRUE}
m <- h2o.deeplearning(x,
                      training_frame = train,
                      autoencoder = TRUE,
                      epochs = 300,
                      model_id = "dp_autoencoder_1layer",
                      
                      train_samples_per_iteration = nrow(train),
                      score_interval = 0,
                      score_duty_cycle = 1.0,
                      stopping_rounds = 15,
                      
                      hidden = c(31), # single layer with 31 inputs
                      activation = "Tanh"
                      )
```

### Single layer autoencoder model analysis
```{r}
score_hist <- as.data.frame(h2o.scoreHistory(m))
score_hist

score_hist %>%
    ggplot(aes(x = (as.numeric(row.names(score_hist))), y = training_rmse)) +
    geom_line() +
    geom_point() +
    xlab("index") +
    ylab("Training RMSE")

# limit x and y axis to check if RMSE is still dropping
score_hist %>%
    ggplot(aes(x = (as.numeric(row.names(score_hist))), y = training_rmse)) +
    geom_line() +
    geom_point() +
    xlim(30, 41) +
    ylim(0.08, 0.095) +
    xlab("index") +
    ylab("Training RMSE")
```
Only 41 epochs. I was hoping it can get close to 300. Try compressing the input from 31 to 20.

```{r cache=TRUE}
m2 <- h2o.deeplearning(x,
                      training_frame = train,
                      autoencoder = TRUE,
                      epochs = 300,
                      model_id = "dp_autoencoder_1layer",
                      
                      train_samples_per_iteration = nrow(train),
                      score_interval = 0,
                      score_duty_cycle = 1.0,
                      stopping_rounds = 15,
                      
                      hidden = c(31), # single layer with 31 inputs
                      activation = "Tanh"
                      )

# also trying out multi layer autoencoder
m3 <- h2o.deeplearning(x,
                      training_frame = train,
                      autoencoder = TRUE,
                      epochs = 300,
                      model_id = "dp_autoencoder_1layer",
                      
                      train_samples_per_iteration = nrow(train),
                      score_interval = 0,
                      score_duty_cycle = 1.0,
                      stopping_rounds = 15,
                      
                      hidden = c(31, 15, 31),
                      activation = "Tanh"
                      )
```

```{r}
m4 <- h2o.deeplearning(x,
                      training_frame = train,
                      autoencoder = TRUE,
                      epochs = 300,
                      model_id = "dp_autoencoder_1layer",
                      
                      train_samples_per_iteration = nrow(train),
                      score_interval = 0,
                      score_duty_cycle = 1.0,
                      stopping_rounds = 15,
                      
                      hidden = c(20, 10, 20),
                      activation = "Tanh"
                      )

m5 <- h2o.deeplearning(x,
                      training_frame = train,
                      autoencoder = TRUE,
                      epochs = 300,
                      model_id = "dp_autoencoder_1layer",
                      
                      train_samples_per_iteration = nrow(train),
                      score_interval = 0,
                      score_duty_cycle = 1.0,
                      stopping_rounds = 15,
                      
                      hidden = c(20),
                      activation = "Tanh"
                      )
```

```{r}
score_hist <- as.data.frame(h2o.scoreHistory(m))
score_hist2 <- as.data.frame(h2o.scoreHistory(m2))
score_hist3 <- as.data.frame(h2o.scoreHistory(m3))
score_hist3 <- as.data.frame(h2o.scoreHistory(m4))
score_hist3 <- as.data.frame(h2o.scoreHistory(m5))
score_hist
score_hist2
score_hist3
score_hist4
score_hist5
```
